{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# from train_orig_multiple_codebooks import VQVAE, get_batch\n",
    "from train_orig_multiple_codebooks import VQVAE, get_batch\n",
    "from config import generate_params\n",
    "# from dataset_generator import generate_random_image\n",
    "from utils import seed_everything\n",
    "\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "seed = 111\n",
    "seed_everything(seed)\n",
    "\n",
    "to_tensor = ToTensor()\n",
    "to_pil = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_dir = 'snapshots'\n",
    "train_name = 'overfit_1codebook'\n",
    "snapshot_path = os.path.join(snapshot_dir, train_name, 'snapshot.tar')\n",
    "\n",
    "snapshot = torch.load(snapshot_path, map_location='cpu')\n",
    "\n",
    "model_params = dict(\n",
    "    input_channels=3,\n",
    "    n_hid=64,\n",
    "    n_init=32,\n",
    "    num_codebooks=1,\n",
    "    codebook_size=2 ** 14,\n",
    "    embedding_dim=32\n",
    ")\n",
    "model = VQVAE(**model_params)\n",
    "model.load_state_dict(snapshot['model'])\n",
    "model.eval()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_batch(batch_size=4, transform=to_tensor, **generate_params)\n",
    "\n",
    "model.train()\n",
    "x_hat, _, _ = model(images)\n",
    "\n",
    "grid = torch.cat([images, x_hat], dim=0)\n",
    "grid = make_grid(grid, nrow=4)\n",
    "grid = to_pil(grid)\n",
    "grid.save('tmp_train_mode.png')\n",
    "\n",
    "model.eval()\n",
    "x_hat, _, _ = model(images)\n",
    "\n",
    "grid = torch.cat([images, x_hat], dim=0)\n",
    "grid = make_grid(grid, nrow=4)\n",
    "grid = to_pil(grid)\n",
    "grid.save('tmp_eval_mode.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "images_in = model.recon_loss.inmap(images)\n",
    "x_hat_in, _, _ = model(images_in)\n",
    "\n",
    "grid = torch.cat([images_in, x_hat_in], dim=0)\n",
    "grid = make_grid(grid, nrow=4)\n",
    "grid = to_pil(grid)\n",
    "grid.save('tmp_train_mode_in.png')\n",
    "\n",
    "images_out = model.recon_loss.unmap(images_in)\n",
    "x_hat_out = model.recon_loss.unmap(x_hat_in)\n",
    "\n",
    "grid = torch.cat([images_out, x_hat_out], dim=0)\n",
    "grid = make_grid(grid, nrow=4)\n",
    "grid = to_pil(grid)\n",
    "grid.save('tmp_train_mode_out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfd8684c7f53546f7fdc85df00f20bbd1266ef94d3e3b65674cf698d02319b94"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
